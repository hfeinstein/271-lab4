---
title: "Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 4"
author: "Heather Feinstein, Ryan Delgado, April Kim"
date: "Fall 2018"
output:
  pdf_document: default
---

# Instructions:

*  $\textbf{Due Date: 12/11/2018 (11:59 p.m. Pacific Time)}$
*  $\textbf{Page limit of the pdf report: 20 (not include title and the table of content page}$
  * Use the margin, linespace, and font size specification below:
    * fontsize=11pt
    * margin=1in
    * line_spacing=single

* Submission:
    * Each group makes one submission to Github; please have one of your team members made the submission
    * Submit 2 files:
        1. A pdf file including the details of your analysis and all the R codes used to produce the analysis. Please do not suppress the codes in your pdf file.
        2. R markdown file used to produce the pdf file
    * Use the following file-naming convensation; fail to do so will receive 10% reduction in the grade:
        * FirstNameLastName1_FirstNameLastName2_FirstNameLastName3_LabNumber.fileExtension
        * For example, if you have three students in the group for Lab Z, and their names are Gerard Kelley, Steve Yang, and Jeffrey Yau, then you should name your file the following
            * GerardKelley_SteveYang_JeffreyYau_LabZ.Rmd
            * GerardKelley_SteveYang_JeffreyYau_LabZ.pdf
    * Although it sounds obvious, please write the name of each members of your group on page 1 of your pdf and Rmd files.

* This lab can be completed in a group of up to 3 students in your session. Students are encouraged to work in a group for the lab.

* For statistical methods that we cover in this course, use only the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you have to provide (1) explanation of why such libraries and functions are used instead and (2) reference to the suppressWarnings(suppressMessages(library documentation. Lacking the explanation and reference to the documentation will result in a score of zero for the corresponding question.

* Students are expected to act with regards to UC Berkeley Academic Integrity.

******************************************************
\newpage

# Description of the Lab

In this lab, you are asked to answer the question **"Do changes in traffic laws affect traffic fatalities?"**  To do so, you will conduct the tasks specified below using the data set *driving.Rdata*, which includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is come with the dataste.


## Introduction

Over the years, individual states have enacted many laws to to help prevent car accident deaths ranging from restricting driver alcohol intake to mandating seatbelts for passengers. These laws have been effective to varying degrees and have often corresponded with demographic changes that make effects more difficult to parse. Determing the effectiveness of driving laws has far reaching policy implications. This lab will explore how different traffic laws among US states and the changes in these laws over times have affected traffic fatalities. We'll explore a dataset of traffic laws and fatalities for the 48 contiguous US states, and apply panel regression techniques to this dataset to find relationships between traffic laws and fatalities.

**Exercises:**

1. Load the data. Provide a description of the basic structure of the dataset, as we have done throughout the semester. Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable *totfatrte* and the potential explanatory variables. You need to write a detailed narrative of your observations of your EDA. *Reminder: giving an "output dump" (i.e. providing a bunch of graphs and tables without description and hoping your audience will interpret them) will receive a zero in this exercise.*

```{r setup, include=FALSE}
library(knitr)
library(car)
library(dplyr)
library(Hmisc)
library(ggplot2)
library(lattice)
library(plm)
library(plyr)
library(corrplot)
library(gridExtra)
library(stargazer)

opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

### Initial Examination

```{r}
load("driving.RData")

#view data and count NA
head(data)
print(paste('The number of NA values in the dataset is ', sum(is.na(data))))
```

**Notes:**

* There are 1200 pooled observations in the dataset, with each observation having 56 variables. There are no instances of NA values in the data, indicating that we have a balanced panel dataset.
* The variables can be divided into several categories:
      1) Dummy variables that indicate what laws are implemented in a particular state for a specific 
      2) Dummy variables for each year in the panel.
      3) Continuous variables that measure fatality counts, and ratios of fatalities to the population. This includes our outcome variable of interest, `totfatrte`
      4) Continuous demographic variables like `unem` (state unemployment rate), `perc14_24` (percentage of population aged 14 to 24).
      5) "Index" variables that indicate which year and state the data corresponds to (aptly named `year` and `state`).  

We'll examine these different variable types separately in our EDA. 

### Exploratory Data Analysis
Let's start by looking at our outcome variable, `totfatrte`. This variable measures the Total Number of Fatalities per 100 thousand individuals in the state/year. We'll first plot a histogram of this variable to understand its distribution:

```{r}
ggplot(data, aes(x=totfatrte)) + 
  geom_histogram(fill='darkgreen', color='black') +
  ggtitle('Total Annual Fatalities per 100k')
```

**Observations:**

* We see that the variable is skewed right. This is typical of zero-bounded variables.
* There are a few outliers where the fatalities are above 50 people per 100k. Let's examine the values of the other variables for those observations to see if there's an explanation.

Let's look at the outlier observations:
```{r}
cols.wo.yrdummies <- c('year', 'state', 'sl55', 'sl65', 'sl70', 'sl75', 'slnone', 'seatbelt', 
                       'minage', 'zerotol', 'gdl', 'bac10', 'bac08', 'perse', 'totfat', 
                       'nghtfat', 'wkndfat', 'totfatpvm', 'nghtfatpvm', 'wkndfatpvm', 
                       'statepop', 'totfatrte', 'nghtfatrte', 'wkndfatrte', 'vehicmiles', 
                       'unem', 'perc14_24', 'sl70plus', 'sbprim', 'sbsecon')

t(data[data$totfatrte > 50, cols.wo.yrdummies])
```

Both observations are in state 51 and occur in the early 1980s. It appears this state's 1980 population (~470k) is around 10% of the average state population that year (~4.67 million). This low state population could explain the large fatality ratios in those years. The other variables for this state in these years don't look particularly noteworthy.

Now let's look at how `totfatrte` changes over time:
```{r fig3, out.width = '45%', fig.show='hold', fig.align='center'}
# Get a list of the years w/o the millenium/century so the x axis looks cleaner.
years.nocent <- data %>% distinct(as.character(year)) %>% lapply(substr, 3, 4) %>% unlist(use.names=FALSE)

ggplot(data, aes(as.factor(year), totfatrte)) +
  geom_boxplot() +
  ggtitle('Total Fatality Rate by Year') +
  xlab('Year') + ylab('Total Fatalities') +
  scale_x_discrete(labels = years.nocent) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1.2))
```

The Total Fatality Rate has declined gradually but not steadily since the 80s. The variance across states remains fairly steady over time. It will be interesting to learn the drivers for this decline later in our analysis.

Let's now look at histograms for the continuous variables in the dataset, `perc14_24`, `unem`, and `vehicmilespc`. Variable definitions:

* **perc14_24** - The percentage of the population between the ages of 14 and 24.
* **unem** - The state-level unemployment rate.
* **vehicmilespc** - The number of vehicle miles per capita. This measures how much driving each person does, on average.

Like before, we'll plot histogramas of these variables to learn their distributions:

```{r}
vehicpc.hist <- ggplot(data, aes(x=vehicmilespc)) + 
  geom_histogram(color='black', fill='green') +
  ggtitle('Vehicle Miles Per \nCapita')

unem.hist <- ggplot(data, aes(x=unem)) + 
  geom_histogram(color='black', fill='purple') +
  ggtitle('Unemployment Rate')

perc.hist <- ggplot(data, aes(x=perc14_24)) + 
  geom_histogram(color='black', fill='red') +
  ggtitle('Pct Pop Aged 14-24')

grid.arrange(perc.hist, unem.hist, vehicpc.hist, ncol=3, nrow=1)
```

These distributions do not appear normally distributed, but nothing sticks out as unusual in them. Similar to the `totfatrte` histogram, we see right-ward skewness in the `unem` and `vehicmilespc`. 

How do these variables change over time?

```{r}
vehicpc.ot <- ggplot(data, aes(x=as.factor(year), y=vehicmilespc)) +
 geom_boxplot(fill='green') +
 ggtitle('Vehicle Miles Per Capita Over Time') +
 xlab('Year') + ylab('Miles/Capita') +
 scale_x_discrete(labels = years.nocent) +
 theme(axis.text.x = element_text(angle = 45, hjust = 1.0))

unem.ot <- ggplot(data, aes(x=as.factor(year), y=unem)) +
 geom_boxplot(fill='purple') +
 ggtitle('Unemployment Rate Over Time') +
 xlab('Year') + ylab('Unemployment') +
  scale_x_discrete(labels = years.nocent) +
 theme(axis.text.x = element_text(angle = 45, hjust = 1.0))

perc.ot <- ggplot(data, aes(x=as.factor(year), y=perc14_24)) +
 geom_boxplot(fill='red') +
 ggtitle('Pct Pop Aged 14-24 Over Time') +
 xlab('Year') + ylab('Miles/Capita') +
  scale_x_discrete(labels = years.nocent) +
 theme(axis.text.x = element_text(angle = 45, hjust = 1.0))

grid.arrange(perc.ot, unem.ot, vehicpc.ot, ncol=2, nrow=2)
```

`perc14_24` and `vehicmilespc` both show a strong trend over time, with `perc14_24` trending downward until stabilizing in the early 90s and `vehicmilespc` steadily trending upward. `unem` appears to show a very weak downward trend over the time period with cylical behavior (i.e. it's higher in times of recession). Both `perc14_24` and `vehicmilespc` have a state that appears to be a persistent outlier. In `perc14_24`, the outlier appears to diverge from the stabilizing trend in the early 90s. The outliers in `vehicmilespc` trend upward with the prevailing trend, so that's not particularly interesting.

How do these variables correlate with `totfatrte`? We'll explore this by visualizing the absolute relationships between `totfatrte` and the relationships between the yearly first-differences. Let's look at scatter plots of the absolute variables first:
```{r}
vmpc.vs.tot <- ggplot(data, aes(x=vehicmilespc, y=totfatrte)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Vehicle miles travelled per capita vs Fatalities') +
  xlab('Vehicle miles travelled') + ylab('Fatalities per 100k population') 

perc.vs.tot <- ggplot(data, aes(x=perc14_24, y=totfatrte)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Percent of Pop 14-24 vs Fatalities') +
  xlab('Percent of Pop 14-24') + ylab('Fatalities per 100k population') 

unem.vs.tot <- ggplot(data, aes(x = unem, y = totfatrte)) + 
  geom_point() + geom_smooth(method = lm) +
  ggtitle("Unemployement vs Fatalities") + xlab("Unemployment Rate") +
  ylab("Fatalities per 100k population")

grid.arrange(vmpc.vs.tot, perc.vs.tot, unem.vs.tot, ncol=2, nrow=2)
```

All of these variables appear to have some correlations with `totfatrte`. What about the relationships between the change in these variables with the outcome variable? We'll find the yearly first difference ($x_{t} - x_{t-1}$), partitioned by state:

```{r}
diffed <- data %>% 
  group_by(state) %>%
  mutate(totfatrte.diff = totfatrte - dplyr::lag(totfatrte, order_by=year),
         unem.diff = unem - dplyr::lag(unem, order_by=year),
         vehicmilespc.diff = vehicmilespc - dplyr::lag(vehicmilespc, order_by=year),
         perc14_24.diff = perc14_24 - dplyr::lag(perc14_24, order_by=year)) %>%
  na.omit() %>%
  select(state, year, totfatrte.diff, unem.diff, vehicmilespc.diff, perc14_24.diff)

vmpcdiff.vs.tot <- ggplot(diffed, aes(x=vehicmilespc.diff, y=totfatrte.diff)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Vehicle miles travelled per capita\n vs Fatalities, change') +
  xlab('Vehicle miles travelled') + ylab('Fatalities per 100k population') 

percdiff.vs.tot <- ggplot(diffed, aes(x=perc14_24.diff, y=totfatrte.diff)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Percent of Pop 14-24\n vs Fatalities, change') +
  xlab('Percent of Pop 14-24') + ylab('Fatalities per 100k population') 

unemdiff.vs.tot <- ggplot(diffed, aes(x = unem.diff, y = totfatrte.diff)) + 
  geom_point() + geom_smooth(method = lm) +
  ggtitle("Unemployement vs\n Fatalities, change") + xlab("Unemployment Rate") +
  ylab("Fatalities per 100k population")

grid.arrange(vmpcdiff.vs.tot, percdiff.vs.tot, unemdiff.vs.tot, ncol=2, nrow=2)
```

The relationships of the first differences of the variables are all positive, but the relationship of the differences between `vehicmilespc` and `totfatrte` appears to be stronger than the relationship of the absolute terms. This may be evidence of unobserved heterogeneity at the state level that's eliminated with the first difference. Let's also look at the relationship between these variables with `totfatrte` when they're de-meaned over time:

```{r}
demeaned <- data %>% 
  group_by(state) %>%
  mutate(totfatrte.mean = mean(totfatrte),
         unem.mean = mean(unem),
         vehicmilespc.mean = mean(vehicmilespc),
         perc14_24.mean = mean(perc14_24)) %>%
  mutate(totfatrte.demean = totfatrte - totfatrte.mean,
         unem.demean = unem - unem.mean,
         vehicmilespc.demean = vehicmilespc - vehicmilespc.mean,
         perc14_24.demean = perc14_24 - perc14_24.mean) %>%
  select(state, year, totfatrte.demean, unem.demean, vehicmilespc.demean, perc14_24.demean)

vmpcdemean.vs.tot <- ggplot(demeaned, aes(x=vehicmilespc.demean, y=totfatrte.demean)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Vehicle miles travelled per capita\n vs Fatalities, demeaned') +
  xlab('Vehicle miles travelled') + ylab('Fatalities per 100k population') 

percdemean.vs.tot <- ggplot(demeaned, aes(x=perc14_24.demean, y=totfatrte.demean)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Percent of Pop 14-24\n vs Fatalities, demeaned') +
  xlab('Percent of Pop 14-24') + ylab('Fatalities per 100k population') 

unemdemean.vs.tot <- ggplot(demeaned, aes(x = unem.demean, y = totfatrte.demean)) + 
  geom_point() + geom_smooth(method = lm) +
  ggtitle("Unemployement vs\n Fatalities, demeaned") + xlab("Unemployment Rate") +
  ylab("Fatalities per 100k population")

grid.arrange(vmpcdemean.vs.tot, percdemean.vs.tot, unemdemean.vs.tot, ncol=2, nrow=2)
```

The correlations still appear to be all positive as before, but the correlations of the de-meaned `vehicmilespc` and `totfatrte` are not as strong as the first-differenced correlations.

Next let's examine some of our discrete variables. The variables we'll focus on are \textcolor{red}{also mention speed limit. forgot that.}

* `bac10` and `bac08`, indicator variables for the legal blood-alcohol (BAC) driving limit 
* `perse`, an indicator variable for the implementation of Per Se Laws.
* `sbprim`, an indicator variable for Primary Seat belt laws (which allows law enforcement officers to ticket drivers for solely not wearing a seatbelt)
* `sbsecon`, an indicator variable for Secondary Seat belt laws (which means law enforcement officers cannot ticket drivers for solely not wearing a seatbelt, but can increase the ticket if stopped for another offense.) 
* `gdl`, an indicator variable for Graduated Driver's Licensing laws. These laws mean that drivers must first drive in a supervised learning period, then progress to an intermediate license before being granted a full driver's license.

For each class of categorical variables, we'll visualize them in three ways:
* An initial histogram to get a sense of the distribution.
* With time series plots of the number of states implementing them over time _and_ their average `totfatrte` over time. This will give us a sense of how they change over time and their effect on `totfatrte` throughout time.
* With box & whisker plots, with `totfatrte` on the y-axis. This will give a clearer depiction of their impact on `totfatrte`.

Let's examine the distribution of the BAC variables:
```{r}
bac08.hist <- ggplot(data, aes(x=bac08)) + 
  geom_histogram() +
  ggtitle('BAC .08')
bac10.hist <- ggplot(data, aes(x=bac10)) + 
  geom_histogram() +
  ggtitle('BAC .10')

grid.arrange(bac08.hist, bac10.hist, ncol=2, nrow=1)
```

We see that these categorical variables actually have values that are between 0 and 1. These are cases where the BAC laws were implemented in the middle of the year. In the modeling stage we'll round these variables to the nearest whole values and turn them into factors so that their coefficients are more easily interpretable.

Let's plot the BAC values over time:
```{r}
data$baclevel <- 'None'
data[(data$bac08 == 1), 'baclevel'] = '.08'
data[(data$bac10 == 1), 'baclevel'] = '.10'
data$baclevel <- as.factor(data$baclevel)

agged.fat.bybacyr <- data %>%
  group_by(baclevel, year) %>%
  dplyr::summarize(totfatrte = mean(totfatrte),  # calculate average total fatality rates
                   count = n())  # number of states in each bac

bac.fatrte.t <- ggplot(agged.fat.bybacyr, aes(x=year, y=totfatrte, colour=baclevel)) +
  geom_line() + ggtitle('Average Total Fatality Rate by BAC Level, over time') +
  ylab('Average Total Fatality Rate')
  
bac.cnt.t <- ggplot(agged.fat.bybacyr, aes(x=year, y=count, colour=baclevel)) +
  geom_line() + ggtitle('State Count by BAC Level, over time') +
  ylab('Number of States')

grid.arrange(bac.fatrte.t, bac.cnt.t, ncol=1, nrow=2)
         
```

We see several things in these plots:

* There's a sharp adoption of BAC laws in the early 1980s, with many states adopting BAC .10 laws.
* States with no BAC laws tend to average higher fatality rates than states with BAC laws.
* states with BAC .08 laws often (but not always) have lower fatality rates than states with .10 laws.
* The number of states with .08 BAC limit laws has risen steadily starting in the late 80s, with the vast majority of states adopting this limit by the end of the sample.
* The average total fatality rate vary wildy in the early 2000s. These extreme values are likely due to sharp declines in the number of states that have these laws rather than true increases in fatality rates in those states. 

Now let's look at boxplot of these different BAC levels vs `totfatrte`:
```{r}
ggplot(data, aes(x=as.factor(baclevel), y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('BAC level vs Fatalities') +
  xlab('BAC level') + ylab('Fatalities per 100k population')
```

This plot shows that cases with more restrictive BAC levels tend to have lower fatality rates. This intuitively makes sense, as the laws may deter intoxicated driving.

Let's now examine `perse`. As before, we'll first plot a histogram:

```{r}
ggplot(data, aes(x=perse)) + 
  geom_histogram() +
  ggtitle('Per Se Law')
```

We again see values between 0 and 1 in this variable. Let's look at how `perse` changes over time:

```{r}
data$persefactor <- as.factor(round(data$perse))

agged.fat.byperseyr <- data %>%
  group_by(persefactor, year) %>%
  dplyr::summarize(totfatrte = mean(totfatrte),  # calculate average total fatality rates
                   count = n())

perse.fatrte.t <- ggplot(agged.fat.byperseyr, aes(x=year, y=totfatrte, colour=persefactor)) +
  geom_line() + ggtitle('Average Total Fatality Rate by perse, over time') +
  ylab('Average Total Fatality Rate')
  
perse.cnt.t <- ggplot(agged.fat.byperseyr, aes(x=year, y=count, colour=persefactor)) +
  geom_line() + ggtitle('State Count by perse, over time') +
  ylab('Number of States')

grid.arrange(perse.fatrte.t, perse.cnt.t, ncol=1, nrow=2)
```

It appears that states without Per Se laws tend to average lower fatality rates over time than states with the laws. This is unexpected, as we'd intuitively assume that more restrictive laws surrounding drunk driving would result in less fatalities. From this visualization, this does not appear to be the case. Let's examine the relationship between `perse` and `totfatrte` further in a box plot:

```{r}
ggplot(data, aes(x=persefactor, y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('Per Se Law vs Fatalities') +
  xlab('Per Se Law?') + ylab('Fatalities per 100k population')
```

Let's move on to looking at the speed limit. There are several speed limit variables in this dataset, `sl55`, `sl65`, `sl70`, `sl75`, `slnone`, and `sl70plus`. The variable names reflect the max speed limit in the state, except for sl70plus, which is a consolidated view of speed limit laws. Let's look at the individual variables:

```{r}
data$slfactor <- 'None'
data[data$sl55 == 1, 'slfactor'] <- '55'
data[data$sl65 == 1, 'slfactor'] <- '65'
data[data$sl70 == 1, 'slfactor'] <- '70'
data[data$sl75 == 1, 'slfactor'] <- '75'
data$slfactor <- as.factor(data$slfactor)

sl.agged <- data %>%
  group_by(slfactor) %>%
  dplyr::summarize(num = n())

ggplot(sl.agged, aes(x=slfactor, y=num)) +
  geom_bar(stat='identity', fill='blue', color='black')
  
  
```

We see that the most common max speed limits are 55 and 65. Given the infrequency of the higher speed limits (<100 in the sample means 3-4 states with these speed limits per year), it may be beneficial to bin these into high & low speed limits. We'll use the sl70plus variable going forward.

Let's look at how speed limits change over time:

```{r}
data$sl70plusfactor <- as.factor(round(data$sl70plus))

agged.fat.bysl <- data %>%
  group_by(sl70plusfactor, year) %>%
  dplyr::summarize(totfatrte = mean(totfatrte),
                   count = n())

sl.fatrte.t <- ggplot(agged.fat.bysl, aes(x=year, y=totfatrte, colour=sl70plusfactor)) +
  geom_line() + ggtitle('Average Total Fatality Rate by Speed Limit, over time') +
  ylab('Average Total Fatality Rate')
  
sl.cnt.t <- ggplot(agged.fat.bysl, aes(x=year, y=count, colour=sl70plusfactor)) +
  geom_line() + ggtitle('State Count by Speed Limit, over time') +
  ylab('Number of States')

grid.arrange(sl.fatrte.t, sl.cnt.t, ncol=1, nrow=2)
```

It appears there was significant adoption of 70+ speed limit laws in the 90s, with the number of 70+ speed limit states stabilizing before 2000. Additionally, it appears that states with higher speed limits have higher fatality rates than states with lower speed limits. This makes sense, as higher speeds likely make drivers more at risk of injury in collisions.


Now let's look at the box plot with `totfatrte`:
```{r}
ggplot(data, aes(x=sl70plusfactor, y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('Speed Limit 70+ vs Fatalities') +
  xlab('SL70+') + ylab('Fatalities per 100k population')
```

The box plot tells the same story: states with 70+ speed limits tend to have higher fatality rates.


Now let's examine the seatbelt law variable, `seatbelt`. We'll consolidate these into one factor variable and create a bar plot:
```{r}
seatbelt.agged <- data %>%
  group_by(seatbelt) %>%
  dplyr::summarize(count = n())

ggplot(seatbelt.agged, aes(x=seatbelt, y=count)) +
  geom_bar(stat='identity', fill='darkgreen', color='black') +
  ggtitle('Frequency count of seatbelt law types')

```

Secondary seatbelt laws are the most frequent. How do these laws change over time?:

```{r}
data$seatbeltfactor <- as.factor(data$seatbelt)

agged.fat.bysb <- data %>%
  group_by(seatbeltfactor, year) %>%
  dplyr::summarize(totfatrte = mean(totfatrte),
                   count = n())

sb.fatrte.t <- ggplot(agged.fat.bysb, aes(x=year, y=totfatrte, colour=seatbeltfactor)) +
  geom_line() + ggtitle('Average Total Fatality Rate by Seatbelt Law, over time') +
  ylab('Average Total Fatality Rate')
  
sb.cnt.t <- ggplot(agged.fat.bysb, aes(x=year, y=count, colour=seatbeltfactor)) +
  geom_line() + ggtitle('State Count by Seatbelt Law, over time') +
  ylab('Number of States')

grid.arrange(sb.fatrte.t, sb.cnt.t, ncol=1, nrow=2)
```

We see that more states have adopted primary or secondary seatbelt laws over time, with the vast majority states having some seatbelt law by 1995. We also see that states with Primary seatbelt laws tend to average lower fatality rates over the years compared to states with secondary seatbelt laws. We see cases were states with no seatbelt laws have lower fatality rates than states with the laws, but that's likely due to the smaller number of states resulting in extreme values.

Let's look at box plots:

```{r}
ggplot(data, aes(x=seatbeltfactor, y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('Seatbelt Law vs Fatalities') +
  xlab('Seatbelt Law') + ylab('Fatalities per 100k population')
```

The box plot also shows that states with seatbelt laws tend to have lower fatality rates, with the more restrictive Primary seatbelt laws resulting in the lowest fatality rates.

Finally, let's examine `gdl`. Bar plot:
```{r}
gdl.agged <- data %>%
  group_by(gdl) %>%
  dplyr::summarize(count = n())

ggplot(gdl.agged, aes(x=gdl, y=count)) +
  geom_bar(stat='identity', fill='darkgreen', color='black') +
  ggtitle('Frequency count of Graduated Driver\'s License laws')
```

We again see non-discrete values for this variable, similar to the BAC and Per Se variables. As before, we'll discretize this variable by rounding. Let's plot the time series graphs:
```{r}
data$gdlfactor <- as.factor(round(data$gdl))

agged.fat.bygdl <- data %>%
  group_by(gdlfactor, year) %>%
  dplyr::summarize(totfatrte = mean(totfatrte),
                   count = n())

gdl.fatrte.t <- ggplot(agged.fat.bygdl, aes(x=year, y=totfatrte, colour=gdlfactor)) +
  geom_line() + ggtitle('Average Total Fatality Rate by GDL, over time') +
  ylab('Average Total Fatality Rate')
  
gdl.cnt.t <- ggplot(agged.fat.bygdl, aes(x=year, y=count, colour=gdlfactor)) +
  geom_line() + ggtitle('State Count by GDL, over time') +
  ylab('Number of States')

grid.arrange(gdl.fatrte.t, gdl.cnt.t, ncol=1, nrow=2)
```

States began to enact Graduated Driver's License laws starting in the late 90s, and we immediately see that states that enact these laws have lower fatality rates. The box plot tells the same story:

```{r}
ggplot(data, aes(x=gdlfactor, y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('GDL vs Fatalities') +
  xlab('Graduated Driver\'s License') + ylab('Fatalities per 100k population')
```

**EDA Key Takeaways:**

* Our continuous variables `vehicmilespc`, `unem`, and `perc14_24` are all positively correlated with `totfatrte`. These correlations also held when first-differencing and de-meaning the variables.
* States with more restrictive BAC laws tended to have lower fatality rates, with more states enacting these laws over time.
* States with Per Se laws tended to have lower fatality rates. These laws have become more common with time.
* States with higher speed limits tended to have higher fatality rates. Higher speed limits have become more common with time.
* States with primary and secondary seatbelt laws tended to have lower fatality rates, with primary seatbelt laws having the lowest fatality rates.
* States with Graduated Driver's Licensing laws tended to have lower fatality rates, and more states have adopted these laws over time.

2. How is the our dependent variable of interest *totfatrte* defined? What is the average of this variable in each of the years in the time period covered in this dataset? Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004. What does this model explain? Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.

*totfatrte* is defined as "fatalities per 100,000 population"

\textcolor{red}{Let's plot this over time. -RD}
```{r}
#avg per year covered in data set
ddply(data, .(year), summarize,  Total=mean(totfatrte))
```

We'll estimate the linear regression model on the year dummies using the `totfatrte` column and the `year` column converted to a factor:

```{r}
#linear model
mod1 <- lm(totfatrte ~ factor(year) , data=data)
summary(mod1)
```

The summary shows that the coefficients are estimated to be negative for each year after 1980, with the coefficients being statistically significant for all years except 1981. This corroborates the observations we made in the box plot of `totfatrte` over time. Does this mean that driving became safer over this time period? The answer to that question is multi-faceted - better driving habits, more/less speeding, frequency of drunk driving, car safety. \textcolor{red}{need to elaborate and reword this portion.}

3. Expand your model in *Exercise 2* by adding variables *bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc*, and perhaps *transformations of some or all of these variables*. Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. How are the variables *bac8* and *bac10* defined? Interpret the coefficients on *bac8* and *bac10*. Do *per se laws* have a negative effect on the fatality rate? What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.) 

\textcolor{blue}{I think we should log transform unem}
\textcolor{red}{Should we include a latex rendering of the model we'll estimate? We may score extra points by doing that... }
\textcolor{blue}{yesssss we def should}

```{r}
data$bac08round <- round(data$bac08)
data$bac10round <- round(data$bac10)
data$sl70plusround <- round(data$sl70plus)
data$perseround <- round(data$perse)
data$sbprimround <- round(data$sbprim)
data$sbseconround = round(data$sbsecon)
data$gdlround = round(data$gdl)

mod2 <- lm(totfatrte ~ factor(year) + bac08round + bac10round + 
             perseround + sbprimround + sbseconround + sl70plusround +
             gdlround + perc14_24 + log(unem) + vehicmilespc, 
           data=data)

summary(mod2)
```

bac8 is blood alcohol limit .08
bac10 is blood alcohol limit .10

This model indicates that in addition to years, blood alcohol limis of either .08 or .1 have a significant impact decreasing fatalities in car accidents. Further, it indicates that speed limits over 70, high unemployment, and high vehicle miles per capita have significant impact increasing fatalities from car accidents. 

4. Reestimate the model from *Exercise 3* using a fixed effects (at the state level) model. How do the coefficients on *bac08, bac10, perse, and sbprim* compare with the pooled OLS estimates? Which set of estimates do you think is more reliable? What assumptions are needed in each of these models?  Are these assumptions reasonable in the current context?

```{r}
model.fe <- plm(totfatrte ~ factor(year) + bac08round + bac10round + 
                perseround + sbprimround + sbseconround + sl70plusround +
                gdlround + perc14_24 + log(unem) + vehicmilespc, 
                data=data, 
                index=c('state', 'year'), model='within')

summary(model.fe)
# stargazer(mod2, model.fe, type = "text")
```
- All four estimates are directionally negative in both models and all four estimates are statistically significant at p<0.001 level in the fixed effect model. *bac08* and *bac10* coefficients from the pooled OLS has higher absolute estimates when compared to those from the fixed effect model, where they are statistically significant at p<0.001 from both models. On the other hand, *perse* and *sbprim* coefficients from the fixed effect model has higher absolute estimates when compared to those from the pooled OLS and are significant only in the fixed effect model.    
- The fixed effect estimates are likely to be more reliable because the standard errors are uniformly lower when compared to those from the pooled OLS, indicating higher precision in the fixed effect model. In addition, the pooled OLS assumes there is independence between the observations and does not account for unobserved heterogeneity, which makes the fixed effect model more consistent compared to the pooled OLS. 
- The fixed effect model assumes that the state fixed effects are time independent and the explanatory variables change over time with no perfect linear relationship between the variables. In comparison, the pooled OLS assumes that the response variable is normally distributed and errors are uncorrelated with the explanatory variables, which are valid assumptions based on the residual diagnostics. 


5. Would you perfer to use a random effects model instead of the fixed effects model you built in *Exercise 4*? Please explain.

To determine whether random effects model should be used over the fixed effects model, we can conducts a Hausman test with the null hypothesis that the preferred model is random effects. 

```{r}
model.re <- plm(totfatrte ~ factor(year) + bac08round + bac10round + 
                perseround + sbprimround + sbseconround + sl70plusround +
                gdlround + perc14_24 + log(unem) + vehicmilespc, 
                data=data, 
                index=c('state', 'year'), model='random')
phtest(model.fe, model.re)
```

With the p-value < 2.2e-16, we can reject the null hypothesis that the random effects assumptions are correct and we would prefer to use the fixed effects model.

6. Suppose that *vehicmilespc*, the number of miles driven per capita, increases by $1,000$. Using the FE estimates, what is the estimated effect on *totfatrte*? Please interpret the estimate.

According to our fixed effect model, the coefficient for $vehicmilespc$ variable was 0.000951 fatalities/100k people per mile driven per capita. For all other things held equal, if, on average, there's an increase of 1,000 miles driven per capita, we would expect an increase of 0.951 (approximately 1) fatalities per 100k people. 

7. If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors?

The fixed effects model assumes that the idiosyncratic errors are uncorrelated. If there is serial correlation in the model errors, the estimated variance will be biased which will result in underestimated standard errors and thus rending most statistical tests invalid. This would most likely commit Type I error, and reject the null hypothesis too easily. 

Heteroskedasticity in the idiosyncratic errors would result in overstated standard errors and may commit Type II error. We may fail to reject the null hypothesis since significance of potentially valuable regressor will not be detected. 












