---
title: "Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 4"
author: "Heather Feinstein, Ryan Delgado, April Kim"
date: "Fall 2018"
output:
  pdf_document: default
---

# Instructions:

*  $\textbf{Due Date: 12/11/2018 (11:59 p.m. Pacific Time)}$
*  $\textbf{Page limit of the pdf report: 20 (not include title and the table of content page}$
  * Use the margin, linespace, and font size specification below:
    * fontsize=11pt
    * margin=1in
    * line_spacing=single

* Submission:
    * Each group makes one submission to Github; please have one of your team members made the submission
    * Submit 2 files:
        1. A pdf file including the details of your analysis and all the R codes used to produce the analysis. Please do not suppress the codes in your pdf file.
        2. R markdown file used to produce the pdf file
    * Use the following file-naming convensation; fail to do so will receive 10% reduction in the grade:
        * FirstNameLastName1_FirstNameLastName2_FirstNameLastName3_LabNumber.fileExtension
        * For example, if you have three students in the group for Lab Z, and their names are Gerard Kelley, Steve Yang, and Jeffrey Yau, then you should name your file the following
            * GerardKelley_SteveYang_JeffreyYau_LabZ.Rmd
            * GerardKelley_SteveYang_JeffreyYau_LabZ.pdf
    * Although it sounds obvious, please write the name of each members of your group on page 1 of your pdf and Rmd files.

* This lab can be completed in a group of up to 3 students in your session. Students are encouraged to work in a group for the lab.

* For statistical methods that we cover in this course, use only the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you have to provide (1) explanation of why such libraries and functions are used instead and (2) reference to the suppressWarnings(suppressMessages(library documentation. Lacking the explanation and reference to the documentation will result in a score of zero for the corresponding question.

* Students are expected to act with regards to UC Berkeley Academic Integrity.

******************************************************
\newpage

# Description of the Lab

In this lab, you are asked to answer the question **"Do changes in traffic laws affect traffic fatalities?"**  To do so, you will conduct the tasks specified below using the data set *driving.Rdata*, which includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is come with the dataste.


## Introduction

Over the years, individual states have enacted many laws to to help prevent car accident deaths ranging from restricting driver alcohol intake to mandating seatbelts for passengers. These laws have been effective to varying degrees and have often corresponded with demographic changes that make effects more difficult to parse. Determing the effectiveness of driving laws has far reaching policy implications. This lab will explore how different traffic laws among US states and the changes in these laws over times have affected traffic fatalities. We'll explore a dataset of traffic laws and fatalities for the 48 contiguous US states, and apply panel regression techniques to this dataset to find relationships between traffic laws and fatalities.

**Exercises:**

1. Load the data. Provide a description of the basic structure of the dataset, as we have done throughout the semester. Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable *totfatrte* and the potential explanatory variables. You need to write a detailed narrative of your observations of your EDA. *Reminder: giving an "output dump" (i.e. providing a bunch of graphs and tables without description and hoping your audience will interpret them) will receive a zero in this exercise.*

```{r setup, include=FALSE}
library(knitr)
library(car)
library(dplyr)
library(Hmisc)
library(ggplot2)
library(lattice)
library(plm)
library(plyr)
library(corrplot)
library(gridExtra)

opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

### Initial Examination

```{r}
load("driving.RData")

#view data and count NA
head(data)
print(paste('The number of NA values in the dataset is ', sum(is.na(data))))
```

```{r}
#variable definitions
desc
```

**Notes:**

* There are 1200 pooled observations in the dataset, with each observation having 56 variables. There are no instances of NA values in the data, indicating that we have a balanced panel dataset.
* The variables can be divided into several categories:
      1) Dummy variables that indicate what laws are implemented in a particular state for a specific 
      2) Dummy variables for each year in the panel.
      3) Continuous variables that measure fatality counts, and ratios of fatalities to the population. This includes our outcome variable of interest, `totfatrte`
      4) Continuous demographic variables like `unem` (state unemployment rate), `perc14_24` (percentage of population aged 14 to 24).
      5) "Index" variables that indicate which year and state the data corresponds to (aptly named `year` and `state`).  

We'll examine these different variable types separately in our EDA. 

### Exploratory Data Analysis
Let's start by looking at our outcome variable, `totfatrte`. We'll plot a histogram of this variable:

```{r}
ggplot(data, aes(x=totfatrte)) + 
  geom_histogram(fill='darkgreen', color='black') +
  ggtitle('Total Annual Fatalities per 100k')
```

**Observations:**

* We see that the dataset is skewed right. This is typical of zero-bounded variables.
* There are a few cases where the fatalities are above 50 people per 100k. We will examine those in more detail to see if there's an explanation

Let's look at the outlier observations:
```{r}
cols.wo.yrdummies <- c('year', 'state', 'sl55', 'sl65', 'sl70', 'sl75', 'slnone', 'seatbelt', 
                       'minage', 'zerotol', 'gdl', 'bac10', 'bac08', 'perse', 'totfat', 
                       'nghtfat', 'wkndfat', 'totfatpvm', 'nghtfatpvm', 'wkndfatpvm', 
                       'statepop', 'totfatrte', 'nghtfatrte', 'wkndfatrte', 'vehicmiles', 
                       'unem', 'perc14_24', 'sl70plus', 'sbprim', 'sbsecon')

t(data[data$totfatrte > 50, cols.wo.yrdummies])
```

Both observations are in state 51 and occur in the early 1980s. It appears this state's 1980 population (~470k) is around 10% of the average state population that year (~4.67 million). This low state population could explain the large fatality ratios in those years. The other variables for this state in these years don't look particularly noteworthy.

Now let's look at how this variable over time:

```{r fig3, out.width = '45%', fig.show='hold', fig.align='center'}

year.labels <- apply()
ggplot(data, aes(as.factor(year), totfatrte)) +
 geom_boxplot() +
 ggtitle('Total Fatality Rate by Year') +
 xlab('Year') + ylab('Total Fatalities') +
 theme(axis.text.x = element_text(angle = 45, hjust = 1.2))
```

The Total Fatality Rate has declined gradually but not steadily since the 80s. The variance across states remains fairly steady over time.

Let's see how the states are concentrated.

```{r fig4, out.width = '45%', fig.show='hold', fig.align='center'}
# totfatrte: total fatalities per 100,000 population
ggplot(data, aes(x = year, y = totfatrte, colour = as.factor(state))) +
  geom_line(alpha = 0.7, show.legend = F) +
  ggtitle("Total fatalities per 100k population") + theme_bw()
```

The states are fairly concentrated and the outliers tend to fall on the high rather than the low side.

Let's now look at histograms for `perc14_24`, `unem`, and `vehicmilespc`:

```{r}
vehicpc.hist <- ggplot(data, aes(x=vehicmilespc)) + 
  geom_histogram(color='black', fill='green') +
  ggtitle('Vehicle Miles Per \nCapita')

unem.hist <- ggplot(data, aes(x=unem)) + 
  geom_histogram(color='black', fill='purple') +
  ggtitle('Unemployment Rate')

perc.hist <- ggplot(data, aes(x=perc14_24)) + 
  geom_histogram(color='black', fill='red') +
  ggtitle('Pct Pop Aged 14-24')

grid.arrange(perc.hist, unem.hist, vehicpc.hist, ncol=3, nrow=1)
```

These distributions look fairly ordinary \textcolor{red}{what does "ordinary" mean in a statistical sense? Maybe we should change the wording here.}. Similar to the `totfatrte` histogram, we see right-ward skewness in the `unem` and `vehicmilespc`. \textcolor{red}{Any other commentary. Maybe we can talk about log transforming these variables to deal with the skewness? Is this a good enough reason to log transform?}

How do these variables change over time?

```{r}
vehicpc.ot <- ggplot(data, aes(x=as.factor(year), y=vehicmilespc)) +
 geom_boxplot(fill='green') +
 ggtitle('Vehicle Miles Per Capita Over Time') +
 xlab('Year') + ylab('Miles/Capita') +
 theme(axis.text.x = element_blank(), axis.ticks.x=element_blank())

unem.ot <- ggplot(data, aes(x=as.factor(year), y=unem)) +
 geom_boxplot(fill='purple') +
 ggtitle('Unemployment Rate Over Time') +
 xlab('Year') + ylab('Unemployment') +
 theme(axis.text.x = element_blank(), axis.ticks.x=element_blank())

perc.ot <- ggplot(data, aes(x=as.factor(year), y=perc14_24)) +
 geom_boxplot(fill='red') +
 ggtitle('Pct Pop Aged 14-24 Over Time') +
 xlab('Year') + ylab('Miles/Capita') +
 #theme(axis.ticks.x=element_blank())  # axis.text.x = element_blank(), 
 theme(axis.text.x = element_text(angle = 45, hjust = 1.2))

grid.arrange(perc.ot, unem.ot, vehicpc.ot, ncol=2, nrow=2)
```

**Notes:**
* `perc14_24` and `vehicmilespc` both show a strong trend over time, with `perc14_24` trending downward until stabilizing in 
\textcolor{red}{Questions I want to answer in the EDA:}

\textcolor{red}{-Is there enough variance in them to be good candidate EVs in the model?}
\textcolor{red}{-Does anything stick out as needing a transformation? e.g. rounding. Explain why this is a good idea.}

```{r fig9, out.width = '45%', fig.show='hold', fig.align='center'}
# vehicmiles               vehicle miles traveled, billions
ggplot(data, aes(x = year, y = vehicmilespc, colour = as.factor(state))) +
  geom_line(alpha = 0.7, show.legend = F) +
  ggtitle("Vehicle miles traveled per capita") + theme_bw()
```
Miles traveled per capita have increased over time.

```{r fig10, out.width = '45%', fig.show='hold', fig.align='center'}
ggplot(data, aes(x=vehicmilespc, y=totfatrte)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Vehicle miles travelled per capita vs Fatalities') +
  xlab('Vehicle miles travelled') + ylab('Fatalities per 100k population') 
```
More miles travelled per capita correlates with more fatalities. 

```{r fig11, out.width = '45%', fig.show='hold', fig.align='center'}
# unem            unemployment rate, percent
ggplot(data, aes(x = year, y = unem, colour = as.factor(state))) +
  geom_line(alpha = 0.7, show.legend = F) +
  ggtitle("Unemployment rate, percent") + theme_bw()
```

Unemployment peaked in early 80s followed by decrease until early 90s; steady decrease from mid 90s.

```{r fig12, out.width = '45%', fig.show='hold', fig.align='center'}
ggplot(data, aes(x=unem, y=totfatrte)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Unemployement vs Fatalities') +
  xlab('Unemployment Rate') + ylab('Fatalities per 100k population') 
```

Higher unemployment correlates with more fatalities.

```{r fig13, out.width = '45%', fig.show='hold', fig.align='center'}
# perc14_24          percent population aged 14 through 24
ggplot(data, aes(x = year, y = perc14_24, colour = as.factor(state))) +
  geom_line(alpha = 0.7, show.legend = F) +
  ggtitle("Percent population aged 14 through 24") + theme_bw()
```

Rapid decrease in percent population aged 14 through 24 until early 90s; slight increase afterwards.

```{r fig14, out.width = '45%', fig.show='hold', fig.align='center'}
ggplot(data, aes(x=perc14_24, y=totfatrte)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Percent of Pop 14-24 vs Fatalities') +
  xlab('Percent of Pop 14-24') + ylab('Fatalities per 100k population') 
```

Higher percent of population 14-24 correlates with more fatalities.

Next lets examine our binary variables.

```{r fig1, out.width = '45%', fig.show='hold', fig.align='center'}
par(mfrow=c(2,2))
hist(data$bac08) 
hist(data$bac10) 
hist(data$perse) 
hist(data$sbprim) 
hist(data$sbsecon) 
hist(data$sl70plus) 
hist(data$gdl) 
```

Variables displays some degree of skewness and may need to transform. Additionally, some variables that appear to be binary have values between 0 and 1 to indicate mid year changes. We will round these variables.

```{r}
# if "1" in bac10, set as 0.1; else set as "0" or "1" in bac08
data$bac_combined <- ifelse(round(data$bac10) > 0, 0.1, 0.08*round(data$bac08))
data$sl_combined <- ifelse(round(data$sl55) > 0, 55, ifelse(round(data$sl65) > 0, 65,
                         ifelse(round(data$sl70) > 0, 70, ifelse(round(data$sl75) > 0, 75, 80))))
```

```{r fig5, out.width = '45%', fig.show='hold', fig.align='center'}
ggplot(data, aes(x = year, y = bac_combined, group = state)) + 
  geom_jitter(alpha = 0.3) + theme_bw() +
  ggtitle("Blood alcohol limit")
```

We noticed over time that fewer states have no BAC limit or 0.1 limit and more have a 0.08 limit.  

```{r fig6, out.width = '45%', fig.show='hold', fig.align='center'}
ggplot(data, aes(x=as.factor(bac_combined), y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('BAC level vs Fatalities') +
  xlab('BAC level') + ylab('Fatalities per 100k population')
```
Fatalities appear to be lowest in states with a 0.08 BAC limit and highest in states with no BAC limit.

```{r fig7, out.width = '45%', fig.show='hold', fig.align='center'}
ggplot(data, aes(x = year, y = sl_combined, group = state)) + 
  geom_jitter(alpha = 0.3) + theme_bw() +
  ggtitle("Speed limit")
```

Over time, speed limits have increased.

```{r fig8, out.width = '45%', fig.show='hold', fig.align='center'}
data$slhigh <- (data$sl70plus == 1)
ggplot(data, aes(x=as.factor(slhigh), y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('Total Fatalities between states that have high vs low speed limit') +
  scale_x_discrete(labels=c('Low', 'High')) +
  xlab('Speed limit level') + ylab('Fatalities per 100k population')
```

```{r fig15, out.width = '45%', fig.show='hold', fig.align='center'}
data$perseround = round(data$perse)
ggplot(data, aes(x = year, y = perseround, group = state)) + 
  geom_jitter(alpha = 0.3) + theme_bw() +
  ggtitle("Per Se Laws")
```
States have increasingly implemented per se laws over time.

```{r fig16, out.width = '45%', fig.show='hold', fig.align='center'}
ggplot(data, aes(x=as.factor(perseround), y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('Per Se Law vs Fatalities') +
  xlab('Per Se Law') + ylab('Fatalities per 100k population')
```
States with no per se laws have higher fatalities.

```{r fig17, out.width = '45%', fig.show='hold', fig.align='center'}
data$gdlround = round(data$gdl)
ggplot(data, aes(x = year, y = gdlround, group = state)) + 
  geom_jitter(alpha = 0.3) + theme_bw() +
  ggtitle("Graduated Drivers License Laws")
```
States have increasingly implemented graduated drivers license laws beginning in the late 90s.

```{r fig18, out.width = '45%', fig.show='hold', fig.align='center'}
ggplot(data, aes(x=as.factor(gdlround), y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('Graduated Drivers License Laws vs Fatalities') +
  xlab('Graduated Drivers License Law') + ylab('Fatalities per 100k population')
```
States with without graduated drivers license laws have higher fatalities.

```{r fig19, out.width = '45%', fig.show='hold', fig.align='center'}
data$sbprimround = round(data$sbprim)
ggplot(data, aes(x = year, y = sbprimround, group = state)) + 
  geom_jitter(alpha = 0.3) + theme_bw() +
  ggtitle("Primary Seat Belt Laws")
```
States have increasingly implemented primary seatbelt laws over time.

```{r fig20, out.width = '45%', fig.show='hold', fig.align='center'}
data$sbprimround = round(data$sbprim)
ggplot(data, aes(x=as.factor(sbprimround), y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('Primary Seatbelt Law vs Fatalities') +
  xlab('Primary Seatbelt Law') + ylab('Fatalities per 100k population')
```
States with no primary seatbelt law have higher fatalities.

```{r fig21, out.width = '45%', fig.show='hold', fig.align='center'}
data$sbseconround = round(data$sbsecon)
ggplot(data, aes(x = year, y = sbprimround, group = state)) + 
  geom_jitter(alpha = 0.3) + theme_bw() +
  ggtitle("Secondary Seat Belt Laws")
```
States have increasingly implemented secondary seatbelt laws over time.

```{r fig22, out.width = '45%', fig.show='hold', fig.align='center'}
data$sbseconround = round(data$sbsecon)
ggplot(data, aes(x=as.factor(sbseconround), y=totfatrte)) + 
  geom_boxplot() +
  ggtitle('Secondary Seatbelt Law vs Fatalities') +
  xlab('Secondary Seatbelt Law') + ylab('Fatalities per 100k population')
```
States with no seconday seatbelt law have higher fatlities but the difference is less than the primary seatbelt laes.

2. How is the our dependent variable of interest *totfatrte* defined? What is the average of this variable in each of the years in the time period covered in this dataset? Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004. What does this model explain? Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.

*totfatrte* is defined as "fatalities per 100,000 population"

```{r}
#avg per year covered in data set
ddply(data, .(year), summarize,  Total=mean(totfatrte))
```

We'll estimate the linear regression model on the year dummies using the `totfatrte` column and the `year` column converted to a factor:

```{r}
#linear model
mod1 <- lm(totfatrte ~ factor(year) , data=data)
summary(mod1)
```

The summary shows that the coefficients are estimated to be negative for each year after 1980, with the coefficients being statistically significant for all years except 1981. This corroborates the observations we made in the box plot of `totfatrte` over time. Does this mean that driving became safer over this time period? The answer to that question is multi-faceted - better driving habits, more/less speeding, frequency of drunk driving, car safety. \textcolor{red}{need to elaborate and reword this portion.}

3. Expand your model in *Exercise 2* by adding variables *bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc*, and perhaps *transformations of some or all of these variables*. Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. How are the variables *bac8* and *bac10* defined? Interpret the coefficients on *bac8* and *bac10*. Do *per se laws* have a negative effect on the fatality rate? What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.) 

\textcolor{blue}{I think we should log transform unem}
\textcolor{red}{Should we include a latex rendering of the model we'll estimate? We may score extra points by doing that... }
\textcolor{blue}{yesssss we def should}

```{r}
data$bac08round = round(data$bac08)
data$bac10round = round(data$bac10)
data$sl70plusround = round(data$sl70plus)

mod2 <- lm(totfatrte ~ factor(year) + bac08round + bac10round + 
             perseround + sbprimround + sbseconround + sl70plusround +
             gdlround + perc14_24 + log(unem) + vehicmilespc, 
           data=data)

summary(mod2)
```

bac8 is blood alcohol limit .08
bac10 is blood alcohol limit .10

This model indicates that in addition to years, blood alcohol limis of either .08 or .1 have a significant impact decreasing fatalities in car accidents. Further, it indicates that speed limits over 70, high unemployment, and high vehicle miles per capita have significant impact increasing fatalities from car accidents. 

4. Reestimate the model from *Exercise 3* using a fixed effects (at the state level) model. How do the coefficients on *bac08, bac10, perse, and sbprim* compare with the pooled OLS estimates? Which set of estimates do you think is more reliable? What assumptions are needed in each of these models?  Are these assumptions reasonable in the current context?

```{r}
data$bac08round <- round(data$bac08)
data$bac10round <- round(data$bac10)
data$sl70plusround <- round(data$sl70plus)
data$perseround <- round(data$perse)
data$sbprimround <- round(data$sbprim)
data$sbseconround = round(data$sbsecon)
data$gdlround = round(data$gdl)

model.fe <- plm(totfatrte ~ factor(year) + bac08round + bac10round + 
                perseround + sbprimround + sbseconround + sl70plusround +
                gdlround + perc14_24 + log(unem) + vehicmilespc, 
                data=data, 
                index=c('state', 'year'), model='within')

summary(model.fe)

```

5. Would you perfer to use a random effects model instead of the fixed effects model you built in *Exercise 4*? Please explain.

To determine whether random effects model should be used over the fixed effects model, we can conducts a Hausman test with the null hypothesis that the preferred model is random effects. 

```{r}
model.re <- plm(totfatrte ~ factor(year) + bac08round + bac10round + 
                perseround + sbprimround + sbseconround + sl70plusround +
                gdlround + perc14_24 + log(unem) + vehicmilespc, 
                data=data, 
                index=c('state', 'year'), model='random')
phtest(model.fe, model.re)
```

With the p-value < 2.2e-16, we can reject the null hypothesis that the random effects assumptions are correct and we would prefer to use the fixed effects model.

6. Suppose that *vehicmilespc*, the number of miles driven per capita, increases by $1,000$. Using the FE estimates, what is the estimated effect on *totfatrte*? Please interpret the estimate.

According to our fixed effect model, the coefficient for $vehicmilespc$ variable was 0.000951 fatalities/100k people per mile driven per capita. For all other things held equal, if, on average, there's an increase of 1,000 miles driven per capita, we would expect an increase of 0.951 (approximately 1) fatalities per 100k people. 

7. If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors?

The fixed effects model assumes that the idiosyncratic errors are uncorrelated. If there is serial correlation in the model errors, the estimated variance will be biased which will result in underestimated standard errors and thus rending most statistical tests invalid. This would most likely commit Type I error, and reject the null hypothesis too easily. 

Heteroskedasticity in the idiosyncratic errors would result in overstated standard errors and may commit Type II error. We may fail to reject the null hypothesis since significance of potentially valuable regressor will not be detected. 












