---
title: "Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 4"
author: "Heather Feinstein, Ryan Delgado, April Kim"
date: "Fall 2018"
output:
  pdf_document: default
---

# Instructions:

*  $\textbf{Due Date: 12/11/2018 (11:59 p.m. Pacific Time)}$
*  $\textbf{Page limit of the pdf report: 20 (not include title and the table of content page}$
  * Use the margin, linespace, and font size specification below:
    * fontsize=11pt
    * margin=1in
    * line_spacing=single

* Submission:
    * Each group makes one submission to Github; please have one of your team members made the submission
    * Submit 2 files:
        1. A pdf file including the details of your analysis and all the R codes used to produce the analysis. Please do not suppress the codes in your pdf file.
        2. R markdown file used to produce the pdf file
    * Use the following file-naming convensation; fail to do so will receive 10% reduction in the grade:
        * FirstNameLastName1_FirstNameLastName2_FirstNameLastName3_LabNumber.fileExtension
        * For example, if you have three students in the group for Lab Z, and their names are Gerard Kelley, Steve Yang, and Jeffrey Yau, then you should name your file the following
            * GerardKelley_SteveYang_JeffreyYau_LabZ.Rmd
            * GerardKelley_SteveYang_JeffreyYau_LabZ.pdf
    * Although it sounds obvious, please write the name of each members of your group on page 1 of your pdf and Rmd files.

* This lab can be completed in a group of up to 3 students in your session. Students are encouraged to work in a group for the lab.

* For statistical methods that we cover in this course, use only the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you have to provide (1) explanation of why such libraries and functions are used instead and (2) reference to the suppressWarnings(suppressMessages(library documentation. Lacking the explanation and reference to the documentation will result in a score of zero for the corresponding question.

* Students are expected to act with regards to UC Berkeley Academic Integrity.

******************************************************
\newpage

# Description of the Lab

In this lab, you are asked to answer the question **"Do changes in traffic laws affect traffic fatalities?"**  To do so, you will conduct the tasks specified below using the data set *driving.Rdata*, which includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is come with the dataste.


## Introduction

Over the years, individual states have enacted many laws to to help prevent car accident deaths ranging from restricting driver alcohol intake to mandating seatbelts for passengers. These laws have been effective to varying degrees and have often corresponded with demographic changes that make effects more difficult to parse. Determing the effectiveness of driving laws has far reaching policy implications. This lab will explore how different traffic laws among US states and the changes in these laws over times have affected traffic fatalities. We'll explore a dataset of traffic laws and fatalities for the 48 contiguous US states, and apply panel regression techniques to this dataset to find relationships between traffic laws and fatalities.

**Exercises:**

##1. Load the data. Provide a description of the basic structure of the dataset, as we have done throughout the semester. Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable *totfatrte* and the potential explanatory variables. You need to write a detailed narrative of your observations of your EDA. *Reminder: giving an "output dump" (i.e. providing a bunch of graphs and tables without description and hoping your audience will interpret them) will receive a zero in this exercise.*

```{r setup, include=FALSE}
library(knitr)
library(car)
library(dplyr)
library(Hmisc)
library(ggplot2)
library(lattice)
library(plm)
library(plyr)
library(corrplot)
library(gridExtra)
library(stargazer)

opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

### Initial Examination

```{r}
load("driving.RData")

#view data and count NA
head(data)
print(paste('The number of NA values in the dataset is ', sum(is.na(data))))
```

**Notes:**

* There are 1200 pooled observations in the dataset, with each observation having 56 variables. There are no instances of NA values in the data, indicating that we have a balanced panel dataset.
* The variables can be divided into several categories:
      1) Dummy variables that indicate what laws are implemented in a particular state for a specific 
      2) Dummy variables for each year in the panel.
      3) Continuous variables that measure fatality counts, and ratios of fatalities to the population. This includes our outcome variable of interest, `totfatrte`
      4) Continuous demographic variables like `unem` (state unemployment rate), `perc14_24` (percentage of population aged 14 to 24).
      5) "Index" variables that indicate which year and state the data corresponds to (aptly named `year` and `state`).  

We'll examine these different variable types separately in our Exploratory Data Analysis. 

### Exploratory Data Analysis
Let's start by looking at our outcome variable, `totfatrte`. This variable measures the Total Number of Fatalities per 100 thousand individuals in the state/year. We'll first plot a histogram of this variable to understand its distribution:

```{r, fig.height=3}
ggplot(data, aes(x=totfatrte)) + 
  geom_histogram(fill='darkgreen', color='black') +
  ggtitle('Total Annual Fatalities per 100k')
```

**Observations:**

* We see that the variable is skewed right. This is typical of zero-bounded variables.
* There are a few outliers where the fatalities are above 50 people per 100k. Let's examine the values of the other variables for those observations to see if there's an explanation.

Let's look at the outlier observations:
```{r}
cols.wo.yrdummies <- c('year', 'state', 'sl55', 'sl65', 'sl70', 'sl75', 'slnone', 'seatbelt', 
                       'minage', 'zerotol', 'gdl', 'bac10', 'bac08', 'perse', 'totfat', 
                       'nghtfat', 'wkndfat', 'totfatpvm', 'nghtfatpvm', 'wkndfatpvm', 
                       'statepop', 'totfatrte', 'nghtfatrte', 'wkndfatrte', 'vehicmiles', 
                       'unem', 'perc14_24', 'sl70plus', 'sbprim', 'sbsecon')

t(data[data$totfatrte > 50, cols.wo.yrdummies])
```

Both observations are in state 51 and occur in the early 1980s. It appears this state's 1980 population (~470k) is around 10% of the average state population that year (~4.67 million). This low state population could explain the large fatality ratios in those years. The other variables for this state in these years don't look particularly noteworthy.

Now let's look at how `totfatrte` changes over time:
```{r fig3, out.width = '75%', fig.show='hold', fig.align='center'}
# Get a list of the years w/o the millenium/century so the x axis looks cleaner.
years.nocent <- data %>% distinct(as.character(year)) %>% lapply(substr, 3, 4) %>% unlist(use.names=FALSE)

ggplot(data, aes(as.factor(year), totfatrte)) +
  geom_boxplot() +
  ggtitle('Total Fatality Rate by Year') +
  xlab('Year') + ylab('Total Fatalities') +
  scale_x_discrete(labels = years.nocent) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1.2))
```

The Total Fatality Rate has declined gradually but not steadily since the 80s. The variance across states remains fairly steady over time. It will be interesting to learn the drivers for this decline later in our analysis.

Let's now look at histograms for the continuous variables in the dataset, `perc14_24`, `unem`, and `vehicmilespc`. Variable definitions:

* **perc14_24** - The percentage of the population between the ages of 14 and 24.
* **unem** - The state-level unemployment rate.
* **vehicmilespc** - The number of vehicle miles per capita. This measures how much driving each person does, on average.

Like before, we'll plot histogramas of these variables to learn their distributions:

```{r, fig.height=3}
vehicpc.hist <- ggplot(data, aes(x=vehicmilespc)) + 
  geom_histogram(color='black', fill='green') +
  ggtitle('Vehicle Miles Per \nCapita')

unem.hist <- ggplot(data, aes(x=unem)) + 
  geom_histogram(color='black', fill='purple') +
  ggtitle('Unemployment Rate')

perc.hist <- ggplot(data, aes(x=perc14_24)) + 
  geom_histogram(color='black', fill='red') +
  ggtitle('Pct Pop Aged 14-24')

grid.arrange(perc.hist, unem.hist, vehicpc.hist, ncol=3, nrow=1)
```

These distributions do not appear normally distributed, but nothing sticks out as unusual in them. Similar to the `totfatrte` histogram, we see right-ward skewness in the `unem` and `vehicmilespc`. 

How do these variables change over time?

```{r}
vehicpc.ot <- ggplot(data, aes(x=as.factor(year), y=vehicmilespc)) +
 geom_boxplot(fill='green') +
 ggtitle('Vehicle Miles Per Capita Over Time') +
 xlab('Year') + ylab('Miles/Capita') +
 scale_x_discrete(labels = years.nocent) +
 theme(axis.text.x = element_text(angle = 45, hjust = 1.0))

unem.ot <- ggplot(data, aes(x=as.factor(year), y=unem)) +
 geom_boxplot(fill='purple') +
 ggtitle('Unemployment Rate Over Time') +
 xlab('Year') + ylab('Unemployment') +
  scale_x_discrete(labels = years.nocent) +
 theme(axis.text.x = element_text(angle = 45, hjust = 1.0))

perc.ot <- ggplot(data, aes(x=as.factor(year), y=perc14_24)) +
 geom_boxplot(fill='red') +
 ggtitle('Pct Pop Aged 14-24 Over Time') +
 xlab('Year') + ylab('Miles/Capita') +
  scale_x_discrete(labels = years.nocent) +
 theme(axis.text.x = element_text(angle = 45, hjust = 1.0))

grid.arrange(perc.ot, unem.ot, vehicpc.ot, ncol=2, nrow=2)
```

`perc14_24` and `vehicmilespc` both show a strong trend over time, with `perc14_24` trending downward until stabilizing in the early 90s and `vehicmilespc` steadily trending upward. `unem` appears to show a very weak downward trend over the time period with cylical behavior (i.e. it's higher in times of recession). Both `perc14_24` and `vehicmilespc` have a state that appears to be a persistent outlier. In `perc14_24`, the outlier appears to diverge from the stabilizing trend in the early 90s. The outliers in `vehicmilespc` trend upward with the prevailing trend, so that's not particularly interesting.

How do these variables correlate with `totfatrte`? We'll explore this by visualizing the absolute relationships between `totfatrte'. Let's look at scatter plots of the absolute variables:
```{r, fig.height=4}
vmpc.vs.tot <- ggplot(data, aes(x=vehicmilespc, y=totfatrte)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Vehicle miles travelled per capita vs Fatalities') +
  xlab('Vehicle miles travelled') + ylab('Fatalities per 100k population') 

perc.vs.tot <- ggplot(data, aes(x=perc14_24, y=totfatrte)) + 
  geom_point() +
  geom_smooth(method=lm) +
  ggtitle('Percent of Pop 14-24 vs Fatalities') +
  xlab('Percent of Pop 14-24') + ylab('Fatalities per 100k population') 

unem.vs.tot <- ggplot(data, aes(x = unem, y = totfatrte)) + 
  geom_point() + geom_smooth(method = lm) +
  ggtitle("Unemployement vs Fatalities") + xlab("Unemployment Rate") +
  ylab("Fatalities per 100k population")

grid.arrange(vmpc.vs.tot, perc.vs.tot, unem.vs.tot, ncol=2, nrow=2)
```

All of these variables appear to have positive correlations with `totfatrte`.

Next let's examine some of our discrete variables. The variables we'll focus on are:

* `bac10` and `bac08`, indicator variables for the legal blood-alcohol (BAC) driving limit 
* `perse`, an indicator variable for the implementation of Per Se Laws.
* `sbprim`, an indicator variable for Primary Seat belt laws (which allows law enforcement officers to ticket drivers for solely not wearing a seatbelt)
* `sbsecon`, an indicator variable for Secondary Seat belt laws (which means law enforcement officers cannot ticket drivers for solely not wearing a seatbelt, but can increase the ticket if stopped for another offense.)
* `sl70plus`, an indicator variable for whether the state allows for 70+ speed limits on its highways.
* `gdl`, an indicator variable for Graduated Driver's Licensing laws. These laws mean that drivers must first drive in a supervised learning period, then progress to an intermediate license before being granted a full driver's license.


Let's first plot the distributions of these categorical variables:
```{r}
bac08.hist <- ggplot(data, aes(x=bac08)) + 
  geom_histogram() +
  ggtitle('BAC .08')
bac10.hist <- ggplot(data, aes(x=bac10)) + 
  geom_histogram() +
  ggtitle('BAC .10')
perse.hist <- ggplot(data, aes(x=perse)) + 
  geom_histogram() +
  ggtitle('Per Se Law')
sl.hist <- ggplot(data, aes(x=sl70plus)) + 
  geom_histogram() +
  ggtitle('Speed Limit 70+ Law')
gdl.hist <- ggplot(data, aes(x=gdl)) + 
  geom_histogram() +
  ggtitle('Graduated Driver\'s License Law')
grid.arrange(bac08.hist, bac10.hist, perse.hist, sl.hist, gdl.hist, ncol=2, nrow=3)
```

In each of the categorical variables we see values between 0 and 1. This is due to states implementing laws intra year. We'll simplify these variables by rounding them to the nearest whole number.

We'll visualize each class of categorical variables in two ways:

* With a time series plot of the number of states implementing them over time
* With a box & whisker plot, with `totfatrte` on the y-axis. This will give a clearer depiction of their impact on `totfatrte`.

Let's start with the BAC law frequencies over time and the `totfatrte` box plot by BAC laws:
```{r, fig.height=3}
data$baclevel <- 'None'
data[(data$bac08 == 1), 'baclevel'] = '.08'
data[(data$bac10 == 1), 'baclevel'] = '.10'
data$baclevel <- as.factor(data$baclevel)

agged.fat.bybacyr <- data %>%
  group_by(baclevel, year) %>%
  dplyr::summarize(totfatrte = mean(totfatrte),  # calculate average total fatality rates
                   count = n())  # number of states in each bac
bac.cnt.t <- ggplot(agged.fat.bybacyr, aes(x=year, y=count, colour=baclevel)) +
  geom_line() + ggtitle('State Count by BAC Level, over time') +
  ylab('Number of States')
bac.boxplot <- ggplot(data, aes(x=baclevel, y=totfatrte, color=baclevel)) + 
  geom_boxplot() +
  ggtitle('BAC level vs Fatalities') +
  xlab('BAC level') + ylab('Fatalities per 100k population')
grid.arrange(bac.cnt.t, bac.boxplot, ncol=2, nrow=1)
```

We see several things in these plots:

* There's a sharp adoption of BAC laws in the early 1980s, with many states adopting BAC .10 laws.
* The number of states with .08 BAC limit laws has risen steadily starting in the late 80s, with the vast majority of states adopting this limit by the end of the sample.
* States with more restrictive BAC laws (i.e. with BAC levels of .08) tend to have lower fatality rates than states with more lax BAC laws.

Let's now examine the same plots for `perse`:

```{r, fig.height=3}
data$persefactor <- as.factor(round(data$perse))
agged.fat.byperseyr <- data %>%
  group_by(persefactor, year) %>%
  dplyr::summarize(totfatrte = mean(totfatrte),  # calculate average total fatality rates
                   count = n())
perse.cnt.t <- ggplot(agged.fat.byperseyr, aes(x=year, y=count, color=persefactor)) +
  geom_line() + ggtitle('State Count by Per se, over time') +
  ylab('Number of States')
perse.boxplot <- ggplot(data, aes(x=persefactor, y=totfatrte, color=persefactor)) + 
  geom_boxplot() +
  ggtitle('Per se Law vs Fatalities') +
  xlab('Per Se Law') + ylab('Fatalities per 100k population')
grid.arrange(perse.cnt.t, perse.boxplot, ncol=2, nrow=1)
```

Per se laws saw steady adoption among states until stabilizing in the late 90s, and  we see that these states tended to average lower fatality rates.


Let's look at how speed limits change over time and how they affect fatality rates:
```{r, fig.height=3}
data$sl70plusfactor <- as.factor(round(data$sl70plus))
agged.fat.bysl <- data %>%
  group_by(sl70plusfactor, year) %>%
  dplyr::summarize(totfatrte = mean(totfatrte),
                   count = n())
sl.cnt.t <- ggplot(agged.fat.bysl, aes(x=year, y=count, color=sl70plusfactor)) +
  geom_line() + ggtitle('State Count by Speed Limit Trend') +
  ylab('Number of States')
sl.boxplot <- ggplot(data, aes(x=sl70plusfactor, y=totfatrte, color=sl70plusfactor)) + 
  geom_boxplot() +
  ggtitle('Speed Limit 70+ vs Fatalities') +
  xlab('SL70+') + ylab('Fatalities per 100k population')
grid.arrange(sl.cnt.t, sl.boxplot, ncol=2, nrow=1)
```

It appears there was significant adoption of 70+ speed limit laws in the 90s, with the number of 70+ speed limit states stabilizing before 2000. Additionally, it appears that states with higher speed limits have higher fatality rates than states with lower speed limits. This makes sense, as higher speeds likely make drivers more at risk of injury in collisions.

What about seatbelt laws?:
```{r, fig.height=3}
data$seatbeltfactor <- as.factor(data$seatbelt)
agged.fat.bysb <- data %>%
  group_by(seatbeltfactor, year) %>%
  dplyr::summarize(totfatrte = mean(totfatrte),
                   count = n())
sb.cnt.t <- ggplot(agged.fat.bysb, aes(x=year, y=count, color=seatbeltfactor)) +
  geom_line() + ggtitle('State Count by Seatbelt Law Trend') +
  ylab('Number of States')
sb.boxplot <- ggplot(data, aes(x=seatbeltfactor, y=totfatrte, color=seatbeltfactor)) + 
  geom_boxplot() +
  ggtitle('Seatbelt Law vs Fatalities') +
  xlab('Seatbelt Law') + ylab('Fatalities per 100k population')
grid.arrange(sb.cnt.t, sb.boxplot, ncol=2, nrow=1)
```

We see that more states have adopted primary or secondary seatbelt laws over time, with the vast majority states having some seatbelt law by 1995. We also see that states with Primary seatbelt laws tend to average lower fatality rates compared to states with secondary seatbelt laws and states with no seatbelt laws.

Let's wrap-up by looking at Graduated Driver's License laws:
```{r, fig.height=3}
data$gdlfactor <- as.factor(round(data$gdl))
agged.fat.bygdl <- data %>%
  group_by(gdlfactor, year) %>%
  dplyr::summarize(totfatrte = mean(totfatrte),
                   count = n())
gdl.cnt.t <- ggplot(agged.fat.bygdl, aes(x=year, y=count, color=gdlfactor)) +
  geom_line() + ggtitle('State Count by GDL,\nover time') +
  ylab('Number of States')
gdl.boxplot <- ggplot(data, aes(x=gdlfactor, y=totfatrte, color=gdlfactor)) + 
  geom_boxplot() +
  ggtitle('GDL vs Fatalities') +
  xlab('Graduated Driver\'s License') + ylab('Fatalities per 100k population')
grid.arrange(gdl.cnt.t, gdl.boxplot, ncol=2, nrow=1)
```

States began to enact Graduated Driver's License laws starting in the late 90s, and states that enact these laws tend to have lower fatality rates.

**EDA Key Takeaways:**

* Our continuous variables `vehicmilespc`, `unem`, and `perc14_24` are all positively correlated with `totfatrte`. These correlations also held when first-differencing and de-meaning the variables.
* States with more restrictive BAC laws tended to have lower fatality rates, with more states enacting these laws over time.
* States with Per Se laws tended to have lower fatality rates. These laws have become more common with time.
* States with higher speed limits tended to have higher fatality rates. Higher speed limits have become more common with time.
* States with primary and secondary seatbelt laws tended to have lower fatality rates, with primary seatbelt laws having the lowest fatality rates.
* States with Graduated Driver's Licensing laws tended to have lower fatality rates, and more states have adopted these laws over time.

##2. How is the our dependent variable of interest *totfatrte* defined? What is the average of this variable in each of the years in the time period covered in this dataset? Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004. What does this model explain? Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.

`totfatrte` is defined as "fatalities per 100,000 population"

```{r}
#avg per year covered in data set
ddply(data, .(year), summarize,  Total=mean(totfatrte))
```

We'll estimate the linear regression model on the year dummies using the `totfatrte` column and the `year` column converted to a factor:
$$
totfatrte_{it} = \beta_0 + \sum_{k=1980}^{2004}\delta_{k}year_k + u_{it}
$$
```{r}
#linear model
mod1 <- lm(totfatrte ~ factor(year) , data=data)
summary(mod1)
```

The summary shows that the coefficients are estimated to be negative for each year after 1980, with statistically significant for all years except 1981. This aligns with what we saw in the box plot of `totfatrte` over time in the EDA. 

##3. Expand your model in *Exercise 2* by adding variables *bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc*, and perhaps *transformations of some or all of these variables*. Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. How are the variables *bac8* and *bac10* defined? Interpret the coefficients on *bac8* and *bac10*. Do *per se laws* have a negative effect on the fatality rate? What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.) 

We chose not to perform any transformations, as the EDA did not show nonlinear relationships between `totfatrte` and the continuous variables. We do see that these variables are skewed right which log transforming could "fix". However this will change the interpretations of the coefficients, and this will only cause problems if it skews the residuals. We'll examine the residuals of the model after estimating it to ensure they're not heavily skewed.

We'll estimate the pooled OLS model with the additional variables:
$$
totfatrte_{it} = \beta_0 + \sum_{k=1980}^{2004}\delta_{k}year_k +  \beta_1 bac08_{it} + \beta_2 bac10_{it} + \beta_3 perse_{it} + \beta_4 sbprim_{it} + \beta_5 sbsecon_{it} + \beta_6 sl70plus_{it} +
$$
$$
\beta_7 gdl_{it} + \beta_8 perc14_24_{it} + \beta_9 unem_{it} + \beta_{10} vehicmlespc_{it} + u_{it}
$$

```{r}
data$bac08round <- round(data$bac08)
data$bac10round <- round(data$bac10)
data$sl70plusround <- round(data$sl70plus)
data$perseround <- round(data$perse)
data$sbprimround <- round(data$sbprim)
data$sbseconround = round(data$sbsecon)
data$gdlround = round(data$gdl)

mod2 <- lm(totfatrte ~ factor(year) + bac08round + bac10round + 
             perseround + sbprimround + sbseconround + sl70plusround +
             gdlround + perc14_24 + unem + vehicmilespc, 
           data=data)

summary(mod2)
```

This model indicates that in addition to years, laws with blood alcohol limis of either .08 or .1 have a significant impact decreasing fatalities in car accidents. Further, it indicates that laws that allow speed limits over 70, high unemployment, and high vehicle miles per capita have significant impact increasing fatalities from car accidents. Per se laws show a negative impact at 0.05 significance.

Let's plot the model residuals:
```{r, fig.height=3}
hist(mod2$residuals, main='Pooled OLS Model Residuals')
```

The residuals of the model do not appear heavily skewed, so the skewness we saw in the continuous variables in the EDA will not negatively impact our inferences.

##4. Reestimate the model from *Exercise 3* using a fixed effects (at the state level) model. How do the coefficients on *bac08, bac10, perse, and sbprim* compare with the pooled OLS estimates? Which set of estimates do you think is more reliable? What assumptions are needed in each of these models?  Are these assumptions reasonable in the current context?

We'll estimate this fixed effects model:

Let
$$
\widetilde{x}_{it} = x_{it} - \overline{x}_i
$$

$$
\widetilde{totfatrte}_{it} = \beta_0 + \sum_{k=1980}^{2004}\delta_{k}year_k +  \beta_1 \widetilde{bac08}_{it} + \beta_2 \widetilde{bac10}_{it} + \beta_3 \widetilde{perse}_{it} + \beta_4 \widetilde{sbprim}_{it} + \beta_5 \widetilde{sbsecon}_{it} + \beta_6 \widetilde{sl70plus}_{it} +
$$
$$
\beta_7 \widetilde{gdl}_{it} + \beta_8 \widetilde{perc14_24}_{it} + \beta_9 \widetilde{unem}_{it} + \beta_{10} \widetilde{vehicmlespc}_{it} + \widetilde{u}_{it}
$$

```{r}
model.fe <- plm(totfatrte ~ factor(year) + bac08round + bac10round + 
                perseround + sbprimround + sbseconround + sl70plusround +
                gdlround + perc14_24 + unem + vehicmilespc, 
                data=data, 
                index=c('state', 'year'), model='within')

summary(model.fe)
```
- All four estimates are directionally negative in both models and all four estimates are statistically significant at p<0.001 level in the fixed effect model. *bac08* and *bac10* coefficients from the pooled OLS has higher absolute estimates when compared to those from the fixed effect model, where they are statistically significant at p<0.001 from both models. On the other hand, *perse* and *sbprim* coefficients from the fixed effect model has higher absolute estimates when compared to those from the pooled OLS and are significant only in the fixed effect model.    
- The fixed effect estimates are likely to be more reliable because the standard errors are uniformly lower when compared to those from the pooled OLS, indicating higher precision in the fixed effect model. In addition, the pooled OLS assumes there is independence between the observations and does not account for unobserved heterogeneity, which makes the fixed effect model more consistent compared to the pooled OLS. 
- The fixed effect model assumes that the state fixed effects are time independent and the explanatory variables change over time with no perfect linear relationship between the variables. In comparison, the pooled OLS assumes that the response variable is normally distributed and errors are uncorrelated with the explanatory variables, which are valid assumptions based on the residual diagnostics. 

##5. Would you perfer to use a random effects model instead of the fixed effects model you built in *Exercise 4*? Please explain.

To determine whether random effects model should be used over the fixed effects model, we can conducts a Hausman test with the null hypothesis that the preferred model is random effects. 

```{r}
model.re <- plm(totfatrte ~ factor(year) + bac08round + bac10round + 
                perseround + sbprimround + sbseconround + sl70plusround +
                gdlround + perc14_24 + unem + vehicmilespc, 
                data=data, 
                index=c('state', 'year'), model='random')
phtest(model.fe, model.re)
```

With the p-value < 2.2e-16, we can reject the null hypothesis that the random effects assumptions are correct and we would prefer to use the fixed effects model.

##6. Suppose that *vehicmilespc*, the number of miles driven per capita, increases by $1,000$. Using the FE estimates, what is the estimated effect on *totfatrte*? Please interpret the estimate.

According to our fixed effect model, the coefficient for $vehicmilespc$ variable was 0.000951 fatalities/100k people per mile driven per capita. For all other things held equal, if, on average, there's an increase of 1,000 miles driven per capita, we would expect an increase of 0.951 (approximately 1) fatalities per 100k people. 

##7. If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors?

The fixed effects model assumes that the idiosyncratic errors are uncorrelated. If there is serial correlation in the model errors, the estimated variance will be biased which will result in underestimated standard errors and thus rending most statistical tests invalid. This would most likely commit Type I error, and reject the null hypothesis too easily. 

Heteroskedasticity in the idiosyncratic errors would result in overstated standard errors and may commit Type II error. We may fail to reject the null hypothesis since significance of potentially valuable regressor will not be detected.